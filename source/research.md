---
layout: page
title: Research
permalink: /research/
---

*"You shall now a word by the company it keeps" (Firth, 1957)*.

I am currently working on different methods of distributional semantics.
I find it fascinating that people (and machines) can learn meaning of words just by their contexts.

Prior to that I was working on Probabilistic Topic Models and particularly non-Bayesian approach of Additive Regularization
that enables meeting multiple requirements for a model in practice.

More broadly, I am interested in Natural Language Processing and Machine Learning techniques.

<br />

<p align="center">
  <img src="{{ site.url }}/images/words_small.jpg"  width="300" height="300">
</p>


<br />

### Events and schools

Giving a talk on ARTM embeddings [[Slides]({{ site.url }}/images/Slides_ETH_final.pdf)] as an academic guest at Thomas Hofmann's group at ETH Zurich (November 2017 - April 2018).

Rep4NLP workshop co-located with [ACL-2017](http://acl2017.org/), Vancouver, Canada, July 30 - August 4.
Regularized Topic Models for Sparse Interpretable Word Embeddings [[Poster]({{ site.url }}/images/PosterToRep4NLP_homepage.pdf)]

[DataFest](http://datafest.ru/4/): data science conference and workshops, Moscow, Russia, February 11-12, 2017.
Vector representations of words and documents [[Video (in Russian)](https://www.youtube.com/watch?v=KEXWC-ICH_Y)].

[DeepHack.Q&A](http://qa.deephack.me/): hackathon on Deep Learning and Q&A systems, Moscow, Russia, February 2016.
Word embeddings and topic models: bridging the gap [[Slides](https://drive.google.com/file/d/0B0PX5JnpNX8yR2JrbU01b3VCY1U/view)].

The 2nd Conference of Yandex School of Data Analysis [Machine Learning: Prospects and
Applications](https://yandexdataschool.com/conference/2015/about), Berlin, Germany, October 5-8 2015.
Linguistic regularization of topic models [[Poster]({{ site.url }}/images/Poster_to_Berlin.pdf)].

The 5th Lisbon Machine Learning School [LxMLS-2015](http://lxmls.it.pt/2015/). Lisbon, Portugal, July 16-23 2015.

Visit to Microsoft Research Cambridge, UK, April 2015. Additive regularization
of topic models and its parallel implementation BigARTM.org [[Slides]({{ site.url }}/images/Microsoft_slides.pdf)].

The 8th Russian Summer School in Information Retrieval [RuSSIR 2014](http://romip.ru/russir2014/), Nizhny
Novgorod, Russia, August 18-22 2014. Additive Regularization for
Learning Interpretable Topic Models [[Poster]({{ site.url }}/images/Poster_to_RUSSIR.pdf)].

<br />

### Publications

##### In English:

A. Potapenko [Probabilistic approach for embedding
              arbitrary features of text]({{ site.url }}/images/potapenko2018aist.pdf).
Analysis of Images, Social networks and Texts (AIST-2018). [[Slides]({{ site.url }}/images/slides_aist.pdf)].

A. Potapenko, A. Popov, K. Vorontsov [Interpretable probabilistic embeddings: bridging the
gap between topic models and neural networks]({{ site.url }}/images/potapenko2017ainl.pdf).
Artificial Intelligence and Natural Language (AINL-2017). [[Slides]({{ site.url }}/images/slides_ainl.pdf)].

Vorontsov K. V., Potapenko A. A. [Additive Regularization of Topic Models](https://link.springer.com/content/pdf/10.1007%2Fs10994-014-5476-6.pdf). Machine Learning Journal, Special Issue “Data Analysis and Intelligent Optimization”– Springer, 2015. Volume 101, Issue 1, Page 303-323. DOI: 10.1007/s10994-014-5476-6.

Vorontsov K. V., Potapenko A. A., Plavin A.V. [Additive Regularization of Topic Models for Topic Selection and Sparse Factorization](http://ai2-s2-pdfs.s3.amazonaws.com/bed6/5ae6f3e2c4949d4531392945eff86e572d8d.pdf). The Third International Symposium On Learning And Data Sciences, April 20-22, 2015, Royal Holloway, University of London, UK. – Springer, A. Gammerman et al. (Eds.): SLDS 2015, LNAI 9047, pp. 193-202.

Vorontsov K. V., Potapenko A. A. [Tutorial on Probabilistic Topic Modeling: Additive Regularization for Stochastic Matrix Factorization](https://pdfs.semanticscholar.org/757c/6b0894db85c69b41f112b6325060020ae358.pdf). Analysis of Images, Social Networks, and Texts. – Springer, 2014, CCIS, vol. 436, pp. 29-46.

Potapenko A. A., Vorontsov K. V.  [Robust PLSA Performs Better Than LDA](https://www.researchgate.net/profile/Konstantin_Vorontsov/publication/262314923_Robust_PLSA_performs_better_than_LDA/links/54e9f3480cf25ba91c814c64.pdf). The 35-th European Conference on Information Retrieval, ECIR-2013, Moscow, Russia, 24-27 March 2013. – LNCS 7814, Springer-Verlag Germany, 2013. Pp. 784-787.

<br />

##### In Russian:

Vorontsov K. V., Potapenko A. A. [Regularization of probabilistic topic models to improve interpretability and determine the number of topics](http://www.dialog-21.ru/digests/dialog2014/materials/pdf/VorontsovKVPotapenkoAA.pdf). International Conference on Computational Linguistics “Dialogue”. – Computational Linguistics and Intellectual Technologies, Moscow, 2014. Pp. 707-719.

Potapenko A. A. Regularization of probabilistic topic model for forming topic kernels. XXI International scientific conference “Lomonosov-2014”. – Moscow: Issuing office of MSU, CMC, 2013. Pp. 80-82.

Vorontsov K. V., Potapenko A. A. [Modifications of Generalized EM-algorithm for Probabilistic Topic Modeling](https://www.hse.ru/pubs/share/direct/document/143780498). Journal of Machine Learning and Data Analysis (ISSN 2223-3792), 2013.

Potapenko A. A. Sparse Probabilistic Topic Models. Theses of the 16-th Russian Conference “Mathematical Methods of Pattern Recognition”, Kazan. – Moscow: MAKS Press, 2013, P. 89.

Vorontsov K. V., Potapenko A. A. [Regularization, Robustness and Sparsity of Probabilistic Topic Models](http://crm-en.ics.org.ru/uploads/crmissues/crm_2012_4/12403.pdf). Computer research and modeling, 2012. V. 4, N 4.Pp. 693-706.

Vorontsov K. V., Potapenko A. A. Robust Sparse Probabilistic Topic Models. The 9-th International Conference “Intellectualization of Information Processing” (IIP-2012), Budva, Montenegro. – Moscow: Torus Press, 2012. Pp. 605-608.
